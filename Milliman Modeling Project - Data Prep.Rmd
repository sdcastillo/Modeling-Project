---
title: "Milliman Modeling Project - Data Prep"
author: "Sam Castillo"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: cerulean
    toc: yes
  bibliography: bibliography.bib
csl: biomed-central.csl
email: castillo.sam.d@gmail.com

---

```{r global options, include = FALSE}
knitr::opts_chunk$set( warning=FALSE, message=FALSE)
library(tidyverse)
packages <- c("gbm", "xgboost", "caret", "tidyr", "ggplot2", "lubridate", "corrplot", "caretEnsemble", "e1071", "ggridges", "forcats", "car", "fastDummies", "glmnet", "ggpubr", "xgboost", "broom", "caTools", "GGally")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
```

#Introduction

This is a modeling exercise project similar to a kaggle competition.  The goal is to build a predictive model.

##Objectives

•	Build a predictive model that predicts the “Amount” variable.
•	The model will be evaluated on the mean absolute error (MAE) between the predicted target and the actual target for the holdout dataset. Some tradeoff between model accuracy and simplicity/interpretability is acceptable as long you can justify your modeling decisions. 
•	Determine what variables are most important for making your model predictions.
•	Prepare a .csv or .txt file containing your prediction for each “ID” in the same format as the sample  provided in sample.txt. 
•	Feel free to use whatever tools available to you to approach the problem. 

Why use MAE instead of root mean square error (RMSE)?  MAE is the mean absolute value difference between the predictions, $\hat{y_i}$, and the target, $y_i$.  This means that positive errors are penalized in the same way as negative errors.

$$\text{MAE} = \frac{1}{n}\sum{|y_i - \hat{y_i}|}$$
Root mean squared error treats positive and negative errors equally, just like MAE, but imposes a harsher penalty outliers, observations where there is a large error.  This is because it takes the square of the error.

$$\text{RMSE} = \sqrt{\frac{1}{n}\sum{(y_i - \hat{y_i})^2}}$$
The takeaway is that outliers will be less of an issue in this analysis than in RMSE were used.

[source](https://towardsdatascience.com/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0)

##Data 

There are two data files.  The `train` file has a target value, `amount`, which we will predict.  The `test` file does not have this label and will be used for final evaluation.  

*	train.txt – training dataset
*	test.txt – holdout dataset 
*	sample.txt – sample of the format for evaluation

```{r}
train_raw <- read_csv("train.txt")
test_raw<- read_csv("test.txt")
sample_submission <- read_csv("sample.txt")
```

There is a reasonably large sample size, with `nrow(train)` in `train.txt`.  All models perform better with a larger sample size, which gives us a lot of freedom.  



There are 34 features, which are all numeric and unnamed.  We also see that there are no missing values, which makes life much easier.

```{r}
head(train_raw , 10)
```

For all data manipulations, we will combine the two data sets together in order to insure equal treatment.

```{r}
# summary(train_raw )
combined <- train_raw %>% 
  mutate(source = "train") %>% 
  rbind(test_raw %>% mutate(Amount = "None", source = "test")) %>% 
  mutate(Amount = as.numeric(Amount),
         log_amount = log(Amount + 1))
```

The first item I look at is the distribution of the target, `Amount`.  This is highly right skewed, and so I take a log transform.  This helps to normalize the shape, which is useful for many modeling applications.  We also see that there are significant spikes.  

```{r}
p1 <- train_raw %>% 
  sample_frac(0.2) %>% 
  ggplot(aes(Amount)) + 
  geom_histogram() + 
  ggtitle("Distribution of Amount") + 
  xlim(0, 100000)

p2 <- train_raw %>% 
  sample_frac(0.2) %>% 
  ggplot(aes(sample = Amount)) + 
  stat_qq() + 
  stat_qq_line() + 
  ggtitle("Empirical Normal Quantiles vs Theoretical Quantiles of Amount")

ggarrange(p1, p2, nrow = 1)
```

A key assumption of many linear models is that the response be normally distributed.  I first try taking the log transform.


```{r}
p1 <- train_raw %>% 
  sample_frac(0.2) %>%
  mutate(log_amount = log(Amount + 1)) %>% 
  ggplot(aes(log_amount)) + 
  geom_histogram() + 
  ggtitle("Distribution of Log of Amount")

p2 <- train_raw %>% 
  sample_frac(0.2) %>%
  mutate(log_amount = log(Amount + 1)) %>% 
  ggplot(aes(sample = log_amount)) + 
  stat_qq() + 
  stat_qq_line() + 
  ggtitle("Empirical Normal Quantiles vs Theoretical Quantiles of Log of Amount")

ggarrange(p1, p2, nrow = 1)
```

This is better but still not perfect.  I notice there is a spike at Amount = 100.100.  There are a few other point masses as well.

```{r}
combined %>% 
  group_by(Amount) %>% 
  summarise(n = n()) %>% 
  arrange(desc(n))
```


```{r}
combined <- combined %>% 
  mutate(spike = as.factor(coalesce(case_when(
           Amount %in% c(100.100, 198.198, 89.089, 999.999) ~ as.character(Amount),
           Amount < 100.100 ~ "zero"
         ), "none")))

p1 <- combined %>% 
  sample_frac(0.2) %>% 
  filter(spike == "none") %>% 
  ggplot(aes(log_amount)) + 
  geom_histogram() + 
  ggtitle("Distribution of Log of Amount")

p2 <- combined %>% 
  sample_frac(0.2) %>% 
  filter(spike == "none") %>% 
  mutate(log_amount = log(Amount + 1)) %>% 
  ggplot(aes(sample = log_amount)) + 
  stat_qq() + 
  stat_qq_line() + 
  ggtitle("Empirical Normal Quantiles vs Theoretical Quantiles of Log of Amount") + 
  xlim(-2.5,3)

ggarrange(p1, p2, nrow = 1)
```


There are point masses in the `Amount` distribution.  There are ~1,200 observations with an Amount value of exactly \$100.10.  

We want to compare the two training and test_rawset distributions to see if they are taken from the same distributions.  Let's compare the medians, 1st quantiles, and 3rd quantiles between the training and test_rawsets.

By comparing the 1st quantile, median, and 3rd quantile, we see that they are indeed from the same distribution.

```{r}
first_quantile <- function(x){quantile(x, 0.25)}
third_quantile <- function(x){quantile(x, 0.25)}

combined %>% 
  group_by(source) %>% 
  select(-Amount, -ID, -log_amount, -spike) %>% 
  summarise_all(funs(first_quantile,
                     median,
                     third_quantile
    )) %>% 
  gather(feature, stat, -source) %>% 
  spread(source, stat) %>% 
  mutate(percent_difference = abs((test - train)/train)) %>%
  arrange(desc(percent_difference))
  
```

All of the distributions are symmetric and centered at zero.  V30 - V33 are uniform.

```{r}
combined %>% 
  select_if(is.numeric) %>% 
  select(-ID) %>% 
  sample_frac(0.2) %>% 
  gather(column, value, 21:33) %>% 
  ggplot(aes(value)) + 
  geom_density() + 
  facet_wrap(vars(column), scales = "free")
```

Let's look at some summary statistics.

We see that the features are already ordered by their standard deviations!

```{r}
combined %>% 
  select_if(is.numeric) %>% 
  select(-ID) %>% 
  summarise_all(sd) %>% 
  gather(feature, value) %>% 
  arrange(desc(value)) 
  
```

#Outlier Analysis

Before trying to standardize the data, we need to deal with outliers.  

##Univariate Outlier Detection

Define an outlier as being above the 99.99th percentile or below the 0.0001st percentile.

```{r}
IQR_outlier <- function(input_column, alpha = 0.001){
  x <- unlist(input_column)
  #return 0 if within 3*IQR and 1 if outside
  upper_bound <- quantile(x, 1 - alpha)
  lower_bound <- quantile(x, alpha)
  is_outlier <- (x < lower_bound | x > upper_bound)
  percent_outlier = mean(is_outlier, na.rm = TRUE)
  return(percent_outlier)
}

```

Look only at points where this is the case. We see that fortunately, all columns have the same number of outliers.   

```{r}
train_raw %>% 
  select(-ID) %>% 
  map_df(IQR_outlier, alpha = 0.001) %>% 
  gather(column, percent_univariate_outlier) %>% 
  arrange(desc(percent_univariate_outlier))
```

```{r}
outlier_summary <- train_raw %>% 
  select(-ID) %>% 
  map_df(function(input_column, alpha = 0.0001){
  x <- unlist(input_column)
  #return 0 if within 3*IQR and 1 if outside
  upper_bound <- quantile(x, 1 - alpha)
  lower_bound <- quantile(x, alpha)
  is_outlier <- (x < lower_bound | x > upper_bound)
  return(is_outlier)
  }) %>% 
  mutate(obs_number = row_number()) %>% 
  gather(feature, outlier, -obs_number) %>% 
  arrange(desc(obs_number)) %>% 
  group_by(obs_number) %>% 
  summarise(total_outliers = sum(outlier)) %>% 
  arrange(desc(total_outliers)) %>% 
  filter(total_outliers>0)
head(outlier_summary)
```

Is there anything strange about this point?

```{r}
train_raw %>% 
  mutate(obs_number = row_number()) %>% 
  #select only outlier points
  dplyr::slice(outlier_summary$obs_number) %>% 
  #add these to the rest of the data
  # rbind(train_raw %>% sample_frac(0.01) %>% mutate(obs_number = 99999999999)) %>% 
  # #add in the number of outliers for each point
  left_join(outlier_summary, by = c("obs_number")) %>% 
  mutate(total_outliers_label = ifelse(total_outliers > 10, ID, NA)) %>% 
  ggplot(aes(V1, V5, size = total_outliers)) + 
  geom_point() + 
  geom_text(aes(label = total_outliers_label, hjust=1, vjust=1)) + 
  ggtitle("High Outlying Points Across Multiple Dimensions")
```

```{r}
train_raw %>% 
  mutate(obs_number = row_number()) %>% 
  #select only outlier points
  dplyr::slice(outlier_summary$obs_number) %>% 
  #add these to the rest of the data
  # rbind(train_raw %>% sample_frac(0.01) %>% mutate(obs_number = 99999999999)) %>% 
  # #add in the number of outliers for each point
  left_join(outlier_summary, by = c("obs_number")) %>% 
  mutate(total_outliers_label = ifelse(total_outliers > 10, ID, NA)) %>% 
  ggplot(aes(V1, V7, size = total_outliers)) + 
  geom_point() + 
  geom_text(aes(label = total_outliers_label, hjust=1, vjust=1))
```


We can look at the assymetry of each feature from the skewness.  Let's look at the top 10 most skewed features in more detail.

```{r}
skewed_features <- combined %>% 
  select_if(is.numeric) %>% 
  select(-ID) %>% 
  summarise_all(skewness) %>% 
  gather(feature, skew) %>% 
  mutate(abs_skew = abs(skew)) %>% 
  arrange(desc(abs_skew)) %>% 
  select(feature) %>% 
  unlist() %>% 
  as.character()

combined %>% 
  select(skewed_features[1:10]) %>% 
  gather(column, value) %>% 
  ggplot(aes(value)) + 
  geom_density() + 
  facet_wrap(vars(column), scales = "free")
```
```{r}
train_raw %>% 
  select(skewed_features[1:10]) %>% 
  dplyr::slice(outlier_summary$obs_number) %>% 
  gather(column, value) %>% 
  ggplot(aes(value)) + 
  geom_density() + 
  facet_wrap(vars(column), scales = "free")
```


##Correlations

These features appear to be independent with the exception of `V15` and `V29`.

```{r}
correlation <- combined %>% 
  select_if(is.numeric) %>% 
  select(-ID) %>% 
  cor()

corrplot(correlation,
         type = "upper")
```

`V15` is a duplicate of `v29`.  After dropping `v29`, let's look again at the correlations.

```{r}
combined %>% 
  sample_frac(0.2) %>% 
  ggplot(aes(V15, V29)) +
  geom_point() + 
  ggtitle("Column 15 is a duplicate of v29")
```


```{r}
correlation <- train_raw %>% 
  select_if(is.numeric) %>% 
  mutate(log_amount = log(Amount + 1)) %>% 
  select(-ID, -V29, -Amount) %>% 
  cor()

corrplot(correlation,
         type = "upper")
```

Which features have the highest correlation with the target `Amount`?

```{r}
top_5_corr_with_amount <- data_frame(feature = paste0("V", 1:33), corr_with_log_amount = as.numeric(correlation[, 'log_amount'])) %>% 
  filter(corr_with_log_amount != 1) %>% 
  mutate(abs_corr_with_amount = abs(corr_with_log_amount)) %>% 
  arrange(desc(abs_corr_with_amount)) %>% 
  top_n(5) %>% 
  select(feature) %>% 
  unlist() %>% 
  as.character()

train_raw %>% 
  sample_frac(0.1) %>% 
  mutate(log_amount = log(Amount + 1)) %>% 
  select(top_5_corr_with_amount, log_amount) %>% 
  ggpairs()
```


##Principal Component Analysis

PCA is a way of reducing the dimensions of a matrix by finding a linear subset of the features which explains most of the variance.

As seen below, most of the variation can be explained by the first principal component.

```{r}
model_matrix <- train_raw %>% dplyr::select(-Amount)
pca <- prcomp(model_matrix, scale = T, center = T)
plot(pca, type = "l")
```

The first PC explains 6% of the total variance.  THe other features each explain about exactly 3%.  This suggests that the data was simulated.  These features are almost perfectly independent.  If they were perfectly independent, each variable would explain 1/33rd percent of the variance, which comes out to 0.0303....  

```{r}
summary(pca)
```


```{r}
train_raw %>% 
  sample_frac(0.2) %>% 
  mutate(log_amount = log(Amount + 1)) %>% 
  select(2:6, log_amount) %>% 
  ggpairs(mapping = aes(fill = log_amount), progress = F)
```


```{r}
train_raw %>% 
  sample_frac(0.1) %>% 
  mutate(log_amount = log(Amount + 1)) %>% 
  select(7:12, log_amount) %>% 
  ggpairs(mapping = aes(fill = log_amount), progress = F)
```


These variables are useless.  Don't use these.

```{r}
train_raw %>% 
  sample_frac(0.1) %>% 
  mutate(log_amount = log(Amount + 1)) %>% 
  select(V13, V28, V29, V30, V31, log_amount) %>% 
  ggpairs(mapping = aes(fill = log_amount), progress = F)
```




